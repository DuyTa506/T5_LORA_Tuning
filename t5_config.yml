model:
  tokenizer_path: "google/mt5-small" # optional path to your trained tokenizer, will default to model size tokenizer
  model_size: "google/mt5-small"
  save_name : "mt5-small-lora"

training:
  epochs: 20
  batch_size: 48
  beam_size: 4
  max_output_length: 128
  optimizer: sophia # sgd  #adamw #sophia
  learning_rate: 0.0002 # if sophia , we need to initialize sightly smaller lr  : 2e-4 vs 3e-4
  weight_decay: 0.02
  reduce_lr_on_bleu_plateau: True
  patience: 3
  reduction_factor: 0.1
  min_lr: 0.00000001
  num_workers_data_gen: 4
  shuffle_data: True
  early_stopping: False
  evaluate_dev: True
  use_cuda: "cuda"
  use_int8 : True

data:
  src_lang: lao
  tgt_lang: vi
  src_train: /home/pvanh/data/thoa54/NMT/data_vi_lo/train_lao.txt
  src_dev: /home/pvanh/data/thoa54/NMT/data_vi_lo/dev_lao.txt
  tgt_train: /home/pvanh/data/thoa54/NMT/data_vi_lo/train_viet.txt
  tgt_dev: /home/pvanh/data/thoa54/NMT/data_vi_lo/dev_viet.txt
  src_prefix: translate Lo to Vi

data_config :
  use_HF : False
  hf_dataset_name: "opus100"
  hf_dataset_config: "en-vi"
  src_prefix": "src"
  src_lang: "en"
  tgt_lang: "vi"


lora:
  is_activate : True
  checkpoint: null # str
  target_modules:
  - "q"
  - "v"
  r: 8 #32 did it compare to 8 ?
  alpha: 64
  lora_dropout : 0.1 # 0.05
  bias : "none"
